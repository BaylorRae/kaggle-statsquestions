{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_questions = pd.read_csv('tagged_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>FullText</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>The Two Cultures: statistics vs. machine learn...</td>\n",
       "      <td>machine-learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>Forecasting demographic census What are some o...</td>\n",
       "      <td>forecasting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>Bayesian and frequentist reasoning in plain En...</td>\n",
       "      <td>bayesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>What is the meaning of p values and t values i...</td>\n",
       "      <td>hypothesis-testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36</td>\n",
       "      <td>Examples for teaching: Correlation does not me...</td>\n",
       "      <td>correlation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                           FullText                 Tag\n",
       "0   6  The Two Cultures: statistics vs. machine learn...    machine-learning\n",
       "1  21  Forecasting demographic census What are some o...         forecasting\n",
       "2  22  Bayesian and frequentist reasoning in plain En...            bayesian\n",
       "3  31  What is the meaning of p values and t values i...  hypothesis-testing\n",
       "4  36  Examples for teaching: Correlation does not me...         correlation"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 1000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Test and Training Data\n",
    "\n",
    "- group questions by tag\n",
    "- split each array of questions by `TEST_SPLIT` for each tag\n",
    "- push each split into `test_set` or `train_set` buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_questions = {k: g[\"FullText\"].tolist() for k, g in tagged_questions.groupby('Tag')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set = []\n",
    "train_set = []\n",
    "\n",
    "for tag, questions in grouped_questions.items():\n",
    "    # only take tags with more than 3 questions\n",
    "    if len(questions) < 3:\n",
    "        continue\n",
    "    \n",
    "    offset = math.ceil(len(questions) * TEST_SPLIT)\n",
    "    \n",
    "    for test_question in questions[:offset]:\n",
    "        test_set.append([test_question, tag])\n",
    "        \n",
    "    for train_question in questions[offset:]:\n",
    "        train_set.append([train_question, tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sequences: 17164\n",
      "train sequences: 67542\n"
     ]
    }
   ],
   "source": [
    "x_test = [i[0] for i in test_set]\n",
    "y_test = [i[1] for i in test_set]\n",
    "\n",
    "x_train = [i[0] for i in train_set]\n",
    "y_train = [i[1] for i in train_set]\n",
    "\n",
    "print(\"test sequences:\", len(x_test))\n",
    "print(\"train sequences:\", len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit text from all questions and tags\n",
    "\n",
    "We need to map our words to integers, similar to `pandas.factorize`, for our entire wordset.\n",
    "\n",
    "```javascript\n",
    "// input\n",
    "[['The lazy dog jumped'], ['The lazy man walked around the dog']]\n",
    "\n",
    "// output\n",
    "[1, 2, 3, 4, 5, 6, 7]\n",
    "```\n",
    "\n",
    "Once we have fitted the text we map our test and training data.\n",
    "\n",
    "```javascript\n",
    "// input\n",
    "[['The lazy dog jumped'], ['The lazy man walked around the dog']]\n",
    "\n",
    "// output\n",
    "[[1, 2, 3, 4], [1, 2, 5, 6, 7, 1, 3]] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "question_tokenizer.fit_on_texts(tagged_questions['FullText'])\n",
    "\n",
    "x_test_sequences = question_tokenizer.texts_to_sequences(x_test)\n",
    "x_train_sequences = question_tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filters='' will avoid splitting tags and creating new words\n",
    "tag_tokenizer = Tokenizer(filters='')\n",
    "tag_tokenizer.fit_on_texts(tagged_questions['Tag'])\n",
    "\n",
    "y_test_sequences = tag_tokenizer.texts_to_sequences(y_test)\n",
    "y_train_sequences = tag_tokenizer.texts_to_sequences(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes: 578\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = np.max(y_train_sequences) + 1\n",
    "print('num classes:', NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (67542, 1000)\n",
      "x_test_shape: (17164, 1000)\n"
     ]
    }
   ],
   "source": [
    "x_train_matrices = question_tokenizer.sequences_to_matrix(x_train_sequences, mode='tfidf')\n",
    "x_test_matrices = question_tokenizer.sequences_to_matrix(x_test_sequences, mode='tfidf')\n",
    "print('x_train shape:', x_train_matrices.shape)\n",
    "print('x_test_shape:', x_test_matrices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_categories shape: (67542, 578)\n",
      "y_test_categories shape: (17164, 578)\n"
     ]
    }
   ],
   "source": [
    "y_train_categories = keras.utils.to_categorical(y_train_sequences, NUM_CLASSES)\n",
    "y_test_categories = keras.utils.to_categorical(y_test_sequences, np.max(y_test_sequences) + 1)\n",
    "print('y_train_categories shape:', y_train_categories.shape)\n",
    "print('y_test_categories shape:', y_test_categories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(MAX_WORDS,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60787 samples, validate on 6755 samples\n",
      "Epoch 1/5\n",
      "60787/60787 [==============================] - 16s - loss: 3.0511 - acc: 0.3899 - val_loss: 12.4806 - val_acc: 0.0033\n",
      "Epoch 2/5\n",
      "60787/60787 [==============================] - 16s - loss: 2.4857 - acc: 0.4520 - val_loss: 14.3563 - val_acc: 0.0027\n",
      "Epoch 3/5\n",
      "60787/60787 [==============================] - 17s - loss: 2.2190 - acc: 0.4862 - val_loss: 15.0709 - val_acc: 0.0038\n",
      "Epoch 4/5\n",
      "60787/60787 [==============================] - 17s - loss: 1.9909 - acc: 0.5202 - val_loss: 15.3845 - val_acc: 0.0025\n",
      "Epoch 5/5\n",
      "60787/60787 [==============================] - 17s - loss: 1.7901 - acc: 0.5513 - val_loss: 15.6904 - val_acc: 0.0036\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_matrices, y_train_categories,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16864/17164 [============================>.] - ETA: 0sTest score: 4.24621257908\n",
      "Test accuracy: 0.391284082964\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_matrices, y_test_categories,\n",
    "                       batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
